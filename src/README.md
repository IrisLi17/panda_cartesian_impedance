# Panda control package (real robot)

## Preparations
1. Install `libfranka`, `franka-ros`. (Skip)
2. Run `cd ~/catkin_ws`, `source devel/setup.bash`, `catkin_make` to compile the ros workspace.
3. Unlock robot from `192.168.1.110`. Activate fci. Leave the browser open, and move to the terminal.
## Launch files
- Reset the robot to start pose: `roslaunch franka_example_controllers move_to_start.launch robot_ip:=192.168.1.110`. 
- `aruco_test.launch`: Run aruco-ros package to verify the realsense camera and single marker detection works properly. Commands to run: `roslaunch panda_cartesian_impedance aruco_test.launch markerId:=xx markerSize:=xx`. `markerSize` is in meters. Open another terminal and run `rosrun image_view image_view image:=/aruco_single/result` to view the result. 
- `panda_realsense_eyeonbase.launch`: Eye-on-base calibration (third-person view camera). Calibration results will be saved to `~/.ros/easy_handeye/panda_eob_calib_eye_on_base.yaml` by default. Stick a marker on the panda hand. Run `roslaunch panda_cartesian_impedance panda_realsense_eyeonbase.launch robot_ip:=192.168.1.110 markerId:=xx markerSize:=xx`. In Rvis window, click `Add` -> `Image` -> `result` and `Add` -> `Pose`. Make sure TF can detect the aruco frame before collecting samples. Switch the robot into guidance mode (white light)to change its pose. Collect ~15 samples then calculate results. Click `save` to write the results to disk.
- `panda_realsense_eyeonhand.launch`: Eye-in-hand calibration. Calibration results will be saved to `~/.ros/easy_handeye/panda_eih_calib_eye_on_hand.yaml` by default. Attach the camera to the panda hand with our 3D printed mount. Stick a marker on the table. The usage is similar to `panda_realsense_eyeonbase.launch`.
- `verify_calibration.launch`: Publish calibrated camera links and aruco frame. `roslaunch panda_cartesian_impedance verify_calibration.launch eye_on_hand:=[false/true] robot_ip:=192.168.1.110 markerId:=xx markerSize:=xx`.
- `test_controller_accuracy.launch`: Measure the accuracy of different cartesian controllers. The most accurate (and the least compliant) controller implemented is called `cartesian_ik_controller`. First reset the robot to start pose, then run `roslaunch panda_cartesian_impedance robot_ip:=192.168.1.110`. The robot will move to 100 different positions at 2Hz. Controller error is measured by the distance between achieved position and commanded position.
- `expert_pick_and_place.launch`: A hand-written pick-up policy to verify marker detection and controller pipeline. `roslaunch panda_cartesian_impedance expert_pick_and_place.launch eye_on_hand:=[false/true] robot_ip:=192.168.1.110 markerId:=xx markerSize:=xx`. The robot should grasp the block, lift it up, drop it down, and hopefully start over again if it can still see the marker.
- `test_rl_observation.launch`: Publish observations, no robot movement. `roslaunch panda_cartesian_impedance test_rl_observation.launch eye_on_hand:=[true/false] robot_ip:=192.168.1.110 use_marker:=[true/false] markerId:=xx markerSize:=xx`. Set `use_marker` to `true` if using low-dimensional states, and `false` if using pixel observations. State-based observation is defined in `scripts/obs_publisher.py` and pixel-based observations in `scripts/obs_pixel_publisher.py`.
- `single_neural_policy.launch`: Deploy a neural policy. `roslaunch panda_cartesian_impedance single_neural_policy.launch eye_on_hand:=[true/false] robot_ip:=192.168.1.110 use_marker:=[true/false] markerId:=xx markerSize:=xx load_path:=[/path/to/scripted/model]`. 